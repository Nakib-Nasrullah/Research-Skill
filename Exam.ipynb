{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOV3ESbfWHF2PsbNA74ffI9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nakib-Nasrullah/Research-Skill/blob/main/Exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#a simple neural network with one output neuron, four inputs and weights, and **biase**"
      ],
      "metadata": {
        "id": "CJX6YtHy32lW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "urX7PxZa7HDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e928fd96-0adf-4f9d-f3a8-3284cbf743b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.8\n"
          ]
        }
      ],
      "source": [
        "#a simple neural network with one output neuron, four inputs and weights, and biase\n",
        "inputs = [1.0, 2.0, 3.0,2.5]\n",
        "weights = [0.2, 0.8, -0.5, 1.0]\n",
        "bias = 2.0\n",
        "output = (inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]+inputs[3]*weights[3]+bias)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Design the previous examples of neural network by adjusting weights and biases randomly.\n"
      ],
      "metadata": {
        "id": "SHguIEqS4DdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Inputs\n",
        "inputs = [1.0, 2.0, 3.0, 2.5]\n",
        "\n",
        "# Randomly generate weights (one weight per input)\n",
        "weights = [random.uniform(-1, 1) for i in range(len(inputs))]\n",
        "\n",
        "# Randomly generate a bias\n",
        "bias = random.uniform(-1, 1)\n",
        "\n",
        "# Calculate output of the neuron\n",
        "output = sum(i*w for i, w in zip(inputs, weights)) + bias\n",
        "\n",
        "print(\"Inputs:\", inputs)\n",
        "print(\"Weights:\", weights)\n",
        "print(\"Bias:\", bias)\n",
        "print(\"Output:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcjV6MWs3-OQ",
        "outputId": "8e49f50d-b60e-400c-c52b-84745fc57238"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: [1.0, 2.0, 3.0, 2.5]\n",
            "Weights: [-0.10529846248416508, -0.4162741507858594, -0.17155353958573838, 0.3754115481168734]\n",
            "Bias: 0.35350245043591855\n",
            "Output: -0.16047606208499698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example-2: A fully connected neural network — every neuron in the current layer has connections to every neuron from the previous."
      ],
      "metadata": {
        "id": "OcMoO_B04jqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let’s say we have a scenario with 3 neurons in a layer and 4 inputs:\n",
        "#\n",
        "inputs = [1, 2, 3, 2.5]\n",
        "weights1 = [0.2, 0.8, -0.5, 1]\n",
        "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
        "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
        "bias1 = 2\n",
        "bias2 = 3\n",
        "bias3 =0.5\n",
        "\n",
        "outputs = [\n",
        "    # Neuron 1:\n",
        "    inputs[0]*weights1[0]+\n",
        "    inputs[1]*weights1[1]+\n",
        "    inputs[2]*weights1[2]+\n",
        "    inputs[3]*weights1[3]+bias1,\n",
        "\n",
        "    # Neuron 2:\n",
        "    inputs[0]*weights2[0]+\n",
        "    inputs[1]*weights2[1]+\n",
        "    inputs[2]*weights2[2]+\n",
        "    inputs[3]*weights2[3]+bias2,\n",
        "\n",
        "    # Neuron 3:\n",
        "    inputs[0]*weights3[0]+\n",
        "    inputs[1]*weights3[1]+\n",
        "    inputs[2]*weights3[2]+\n",
        "    inputs[3]*weights3[3]+bias3]\n",
        "\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osmCuEFU4ZLz",
        "outputId": "3820d10a-038d-4a4d-ec42-452986a061d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.8, 1.21, 2.385]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise**: A fully connected neural network — every neuron in the current layer has connections to every neuron from the previous layer. You have 4 input neurons, one hidden layer consisting of 3 neurons and one output neuron. You should adust weights and biases randomly."
      ],
      "metadata": {
        "id": "oT9P6FC15Bat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize inputs (4 neurons in input layer)\n",
        "inputs = np.array([1, 2, 3, 2.5])\n",
        "\n",
        "# Random weights for the connections between input and hidden layer (4 input neurons, 3 hidden neurons)\n",
        "weights_input_hidden = np.random.randn(4, 3)\n",
        "\n",
        "# Random biases for hidden layer (3 neurons in the hidden layer)\n",
        "bias_hidden = np.random.randn(3)\n",
        "\n",
        "# Random weights for the connections between hidden layer and output layer (3 hidden neurons, 1 output neuron)\n",
        "weights_hidden_output = np.random.randn(3, 1)\n",
        "\n",
        "# Random bias for the output layer\n",
        "bias_output = np.random.randn(1)\n",
        "\n",
        "# Activation function (ReLU)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Forward pass for hidden layer\n",
        "hidden_layer_input = np.dot(inputs, weights_input_hidden) + bias_hidden\n",
        "hidden_layer_output = relu(hidden_layer_input)\n",
        "\n",
        "# Forward pass for output layer\n",
        "output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
        "output_layer_output = relu(output_layer_input)\n",
        "\n",
        "print(f\"Input: {inputs}\")\n",
        "print(f\"Hidden layer output: {hidden_layer_output}\")\n",
        "print(f\"Output layer output: {output_layer_output}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jo6gxMzF5Clm",
        "outputId": "5dbe37aa-b8d1-4e06-be3a-c543cc2fe413"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [1.  2.  3.  2.5]\n",
            "Hidden layer output: [0.         0.         3.79416654]\n",
            "Output layer output: [6.00227052]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Initialization of Inputs and Random Parameters\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import numpy as np: This imports the NumPy library, which is essential for numerical operations in Python, especially for matrix manipulations. We’ll use it for matrix multiplication (np.dot()), random number generation, and other operations.\n",
        "\n",
        "# Initialize inputs (4 neurons in input layer)\n",
        "inputs = np.array([1, 2, 3, 2.5])\n",
        "\n",
        "\n",
        "inputs = np.array([1, 2, 3, 2.5]):\n",
        "\n",
        "This represents the input vector with 4 values. These are the values fed into the neural network. For simplicity, the values are set to [1, 2, 3, 2.5], but in real-world scenarios, these could be feature values of a dataset.\n",
        "\n",
        "# Random weights for the connections between input and hidden layer (4 input neurons, 3 hidden neurons)\n",
        "weights_input_hidden = np.random.randn(4, 3)\n",
        "\n",
        "\n",
        "weights_input_hidden = np.random.randn(4, 3):\n",
        "\n",
        "This initializes the weights between the input layer and the hidden layer.\n",
        "\n",
        "There are 4 input neurons and 3 hidden neurons. Hence, the weight matrix is of shape (4, 3), meaning it has 4 rows (one for each input neuron) and 3 columns (one for each hidden neuron).\n",
        "\n",
        "np.random.randn(4, 3) generates random numbers from a standard normal distribution (mean = 0, standard deviation = 1) for each weight.\n",
        "\n",
        "# Random biases for hidden layer (3 neurons in the hidden layer)\n",
        "bias_hidden = np.random.randn(3)\n",
        "\n",
        "\n",
        "bias_hidden = np.random.randn(3):\n",
        "\n",
        "This initializes the bias values for the hidden layer. There are 3 hidden neurons, so the bias vector has 3 values.\n",
        "\n",
        "Each bias is initialized randomly, again using the standard normal distribution.\n",
        "\n",
        "# Random weights for the connections between hidden layer and output layer (3 hidden neurons, 1 output neuron)\n",
        "weights_hidden_output = np.random.randn(3, 1)\n",
        "\n",
        "\n",
        "weights_hidden_output = np.random.randn(3, 1):\n",
        "\n",
        "This initializes the weights between the hidden layer and the output layer.\n",
        "\n",
        "There are 3 hidden neurons and 1 output neuron, so the weight matrix has a shape of (3, 1).\n",
        "\n",
        "# Random bias for the output layer\n",
        "bias_output = np.random.randn(1)\n",
        "\n",
        "\n",
        "bias_output = np.random.randn(1):\n",
        "\n",
        "This initializes the bias for the output layer. Since there is only 1 output neuron, we have a single random bias value.\n",
        "\n",
        "Step 2: Define the ReLU Activation Function\n",
        "# Activation function (ReLU)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu(x): defines the ReLU (Rectified Linear Unit) activation function.\n",
        "\n",
        "np.maximum(0, x): The ReLU function outputs the maximum of 0 or the input x. It simply replaces any negative values with 0, while leaving positive values unchanged.\n",
        "\n",
        "This function is commonly used in neural networks because of its simplicity and ability to help mitigate the vanishing gradient problem during backpropagation.\n",
        "\n",
        "Step 3: Forward Pass Through the Network\n",
        "\n",
        "Now, we compute the outputs by performing a forward pass through the network. This involves calculating the activations of the hidden layer neurons and the output layer neuron.\n",
        "\n",
        "Hidden Layer\n",
        "# Forward pass for hidden layer\n",
        "hidden_layer_input = np.dot(inputs, weights_input_hidden) + bias_hidden\n",
        "\n",
        "\n",
        "hidden_layer_input = np.dot(inputs, weights_input_hidden) + bias_hidden:\n",
        "\n",
        "np.dot(inputs, weights_input_hidden): This performs a dot product between the input vector (inputs) and the weight matrix (weights_input_hidden), which calculates the weighted sum for each hidden neuron.\n",
        "\n",
        "After the dot product, we add the bias for each hidden neuron (i.e., bias_hidden).\n",
        "\n",
        "The result is the input to the hidden layer neurons.\n",
        "\n",
        "hidden_layer_output = relu(hidden_layer_input)\n",
        "\n",
        "\n",
        "hidden_layer_output = relu(hidden_layer_input): This applies the ReLU activation function to the input of the hidden layer, transforming the input into the output of the hidden layer neurons.\n",
        "\n",
        "Output Layer\n",
        "# Forward pass for output layer\n",
        "output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
        "\n",
        "\n",
        "output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output:\n",
        "\n",
        "Here, we calculate the weighted sum for the output neuron by performing a dot product between the output of the hidden layer (hidden_layer_output) and the weight matrix (weights_hidden_output).\n",
        "\n",
        "We add the bias for the output layer (bias_output) to this sum.\n",
        "\n",
        "output_layer_output = relu(output_layer_input)\n",
        "\n",
        "\n",
        "output_layer_output = relu(output_layer_input): Finally, we apply the ReLU activation to the output layer's input, resulting in the final output of the neural network.\n",
        "\n",
        "Step 4: Print the Results\n",
        "print(f\"Input: {inputs}\")\n",
        "print(f\"Hidden layer output: {hidden_layer_output}\")\n",
        "print(f\"Output layer output: {output_layer_output}\")\n",
        "\n",
        "\n",
        "print(f\"Input: {inputs}\"): Prints the input values.\n",
        "\n",
        "print(f\"Hidden layer output: {hidden_layer_output}\"): Prints the output from the hidden layer neurons.\n",
        "\n",
        "print(f\"Output layer output: {output_layer_output}\"): Prints the final output from the output layer."
      ],
      "metadata": {
        "id": "yd4HdlLO-ybt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example-3: Using loop option to get the weights, inputs and biases then feed into the neurons."
      ],
      "metadata": {
        "id": "i5efUhZW-5W1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is more complex then previous one. It is variation of the previous code.\n",
        "# I use a loop to get the weights, inputs and biases then feed into the neurons.\n",
        "inputs = [1, 2, 3, 2.5]\n",
        "weights = [\n",
        "    [0.2, 0.8, -0.5, 1],\n",
        "    [0.5, -0.91, 0.26, -0.5],\n",
        "    [-0.26, -0.27, 0.17, 0.87]\n",
        "]\n",
        "biases = [2, 3, 0.5]\n",
        "\n",
        "# Output of the current layer\n",
        "layer_outputs = []\n",
        "\n",
        "\n",
        "# For each neuron\n",
        "for neuron_weights, neuron_bias in zip(weights, biases):\n",
        "    # Zeroed output of the given neuron\n",
        "    neuron_output = 0\n",
        "\n",
        "    # For each input and weight to the neuron\n",
        "    for n_input, weight in zip(inputs, neuron_weights):\n",
        "        # Multiply this input by the associated weight\n",
        "        # and add to the neuron's output variable\n",
        "\n",
        "        neuron_output += n_input * weight\n",
        "\n",
        "    # Add bias\n",
        "    neuron_output += neuron_bias\n",
        "\n",
        "    # Put the neuron's result into the layer's output list\n",
        "    layer_outputs.append(neuron_output)\n",
        "\n",
        "#print(layer_outputs)\n",
        "w_out=[0.2, 0.8, -0.5]\n",
        "biase_out=0.5\n",
        "final_output=(layer_outputs[0]*w_out[0]+layer_outputs[1]*w_out[1]+layer_outputs[2]*w_out[2])+biase_out\n",
        "print(final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pva21nDj446m",
        "outputId": "bf0e1d25-d290-4ae2-a61e-651f79d682b3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example-4: Using numpy library to calculate dot product in the neural network."
      ],
      "metadata": {
        "id": "QyFTMzz1AFfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# previous probrams with numpy library, to make it faster and easily programmable.\n",
        "import numpy as np\n",
        "\n",
        "inputs = [1.0, 2.0, 3.0, 2.5]\n",
        "weights = [0.2, 0.8, -0.5, 1.0]\n",
        "bias = 2.0\n",
        "\n",
        "outputs = np.dot(weights, inputs) + bias\n",
        "print(outputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9AevEu7-9IV",
        "outputId": "f1f92f72-3d04-4502-9470-0724213468ed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy library to make the complex code into simple one.\n",
        "inputs = [1, 2, 3, 2.5]\n",
        "weights = [\n",
        "    [0.2, 0.8, -0.5, 1],\n",
        "    [0.5, -0.91, 0.26, -0.5],\n",
        "    [-0.26, -0.27, 0.17, 0.87]\n",
        "]\n",
        "biases = [2, 3, 0.5]\n",
        "\n",
        "outputs = np.dot(weights, inputs) + biases\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZYynCEC5A1d",
        "outputId": "b72889ba-129b-48e5-acee-8c608c90b3b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.8   1.21  2.385]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-XpZM_EJAWqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: take inputs containing 3 samples with 4 features, and 3 neurons with weights and biases, respectively. Then print the outputs of the neorons.\n",
        "\n"
      ],
      "metadata": {
        "id": "e6YOYdRIB1R5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "soa-j-Q0SNHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbac3d7f",
        "outputId": "8af80e2e-2952-4ab0-ad00-bd465d5e3b33"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Inputs: 3 samples, each with 4 features\n",
        "# Each row represents a sample, each column a feature\n",
        "inputs = np.array([\n",
        "    [1.0, 2.0, 3.0, 2.5],\n",
        "    [2.0, 5.0, -1.0, 2.0],\n",
        "    [-1.5, 2.7, 3.3, -0.8]\n",
        "])\n",
        "\n",
        "# Weights: for 3 neurons, each connected to 4 features\n",
        "# Each row represents weights for one neuron\n",
        "# The number of columns should match the number of features in inputs\n",
        "weights = np.array([\n",
        "    [0.2, 0.8, -0.5, 1.0],\n",
        "    [0.5, -0.91, 0.26, -0.5],\n",
        "    [-0.26, -0.27, 0.17, 0.87]\n",
        "])\n",
        "\n",
        "# Biases: one bias for each of the 3 neurons\n",
        "biases = np.array([2.0, 3.0, 0.5])\n",
        "\n",
        "# Calculate outputs using dot product and add biases\n",
        "# np.dot(inputs, weights.T) performs the matrix multiplication for all samples and neurons\n",
        "# The .T (transpose) is used because numpy's dot product expects the second matrix to have columns matching the first matrix's rows.\n",
        "# Here, each row of 'weights' corresponds to a neuron's weights, and we want to multiply it by each sample (row of 'inputs').\n",
        "# So, transposing 'weights' aligns it correctly for a direct dot product with 'inputs'.\n",
        "outputs = np.dot(inputs, weights.T) + biases\n",
        "\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nWeights:\\n\", weights)\n",
        "print(\"\\nBiases:\\n\", biases)\n",
        "print(\"\\nOutputs of the neurons:\\n\", outputs)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " [[ 1.   2.   3.   2.5]\n",
            " [ 2.   5.  -1.   2. ]\n",
            " [-1.5  2.7  3.3 -0.8]]\n",
            "\n",
            "Weights:\n",
            " [[ 0.2   0.8  -0.5   1.  ]\n",
            " [ 0.5  -0.91  0.26 -0.5 ]\n",
            " [-0.26 -0.27  0.17  0.87]]\n",
            "\n",
            "Biases:\n",
            " [2.  3.  0.5]\n",
            "\n",
            "Outputs of the neurons:\n",
            " [[ 4.8    1.21   2.385]\n",
            " [ 8.9   -1.81   0.2  ]\n",
            " [ 1.41   1.051  0.026]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example-5: inputs-batch with weights and then transpose them to calculate dot product.\n"
      ],
      "metadata": {
        "id": "Em9i4uaCCLKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs-batch with weights and then transpose them to calculate dot product.\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "inputs = np.array([[1.0, 2.0, 3.0, 2.5],\n",
        "                   [2.0, 4.0, 1.5, 2.0],\n",
        "                   [0.5, 1.5, 2.0, 3.5],\n",
        "                   [0.5, 1.5, 2.0, 3.5],\n",
        "                   [0.5, 1.5, 2.0, 3.5]])\n",
        "\n",
        "weights = np.array([[0.2, 0.8, -0.5, 1.0],\n",
        "                    [0.5, -0.9, 0.3, -0.7],\n",
        "                    [-0.1, 0.6, -0.3, 0.9]])\n",
        "\n",
        "biases = np.array([2.0, 1.0, -0.5])\n",
        "\n",
        "outputs = np.dot(inputs, weights.T)+biases\n",
        "print(outputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgUa59IdSPOv",
        "outputId": "98231e88-6644-4898-c6e3-c8495a622e94"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4.8  -1.15  1.95]\n",
            " [ 6.85 -2.55  3.05]\n",
            " [ 5.8  -1.95  2.9 ]\n",
            " [ 5.8  -1.95  2.9 ]\n",
            " [ 5.8  -1.95  2.9 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61dbeacb-6f76-42bb-80bc-3da75afdccc4",
        "id": "3glPdIrXCsF-"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "inputs = np.array([\n",
        "    [1.0, 2.0, 3.0, 2.5],\n",
        "    [2.0, 4.0, 1.5, 2.0],\n",
        "    [0.5, 1.5, 2.0, 3.5]\n",
        "])\n",
        "\n",
        "weights = np.array([\n",
        "    [0.2, 0.8, -0.5, 1.0],\n",
        "    [0.5, -0.9, 0.3, -0.7],\n",
        "    [-0.1, 0.6, -0.3, 0.9]\n",
        "])\n",
        "\n",
        "# Assuming no biases are provided for this specific calculation, or they are implicitly zero.\n",
        "# As before, we transpose the weights matrix to align dimensions correctly for the dot product.\n",
        "outputs = np.dot(inputs, weights.T)\n",
        "\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nWeights:\\n\", weights)\n",
        "print(\"\\nOutputs:\\n\", outputs)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " [[1.  2.  3.  2.5]\n",
            " [2.  4.  1.5 2. ]\n",
            " [0.5 1.5 2.  3.5]]\n",
            "\n",
            "Weights:\n",
            " [[ 0.2  0.8 -0.5  1. ]\n",
            " [ 0.5 -0.9  0.3 -0.7]\n",
            " [-0.1  0.6 -0.3  0.9]]\n",
            "\n",
            "Outputs:\n",
            " [[ 2.8  -2.15  2.45]\n",
            " [ 4.85 -3.55  3.55]\n",
            " [ 3.8  -2.95  3.4 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example-6: Two hidden layers with four neurons at the first layer and three in the second."
      ],
      "metadata": {
        "id": "FvFPgLMzSdpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# two hidden layers with three neurons at the first layer and three in the second.\n",
        "import numpy as np\n",
        "inputs = [[1, 2, 3, 2.5],\n",
        "          [2., 5., -1., 2],\n",
        "          [-1.5, 2.7, 3.3, -0.8]\n",
        "          ]\n",
        "\n",
        "weights = [[0.2, 0.8, -0.5, 1],\n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]]\n",
        "biases = [2, 3, 0.5]\n",
        "weights2 = [[0.1, -0.14, 0.5],\n",
        "            [-0.5, 0.12, -0.33],\n",
        "            [-0.44, 0.73, -0.13]]\n",
        "biases2 = [-1, 2, -0.5]\n",
        "layer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
        "\n",
        "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n",
        "print(layer2_outputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beaVIT73Cipe",
        "outputId": "717a5fe5-fbaf-466e-9c77-fbe0b7854fad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.5031  -1.04185 -2.03875]\n",
            " [ 0.2434  -2.7332  -5.7633 ]\n",
            " [-0.99314  1.41254 -0.35655]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example-7: Activation functions."
      ],
      "metadata": {
        "id": "r7P8KQIbUkYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmlidal activatino function\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Example usage\n",
        "x = np.array([0.5, -1.0, 2.0])\n",
        "output = sigmoid(x)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfhcnye5CwWP",
        "outputId": "67cd9618-cf9b-43b1-ead1-1bd771ca3907"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.62245933 0.26894142 0.88079708]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The Rectified Linear Unit (ReLU) activation function\n",
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Example usage\n",
        "x = np.array([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
        "output = relu(x)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oIZSFKgUoKf",
        "outputId": "4a747c65-ccbe-4d22-8108-1067fd811bb2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 1. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For solving logic gates, the activation function that is commonly used is the step function or the binary step function. The step function is a threshold-based function that outputs 0 if the input is less than or equal to a certain threshold and outputs 1 if the input is greater than the threshold."
      ],
      "metadata": {
        "id": "6EFIbwosWL4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will use it to solve logic gates problems.\n",
        "def step_function(x, threshold=0):\n",
        "    if x <= threshold:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "# Example usage\n",
        "x = 0.5\n",
        "output = step_function(x)\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afzt4qdBV8Xp",
        "outputId": "d3133105-24a3-4081-ed15-96d34ebcf376"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example-8: Solve AND gate problem using neural network."
      ],
      "metadata": {
        "id": "8BAqgrZrWaR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the inputs and corresponding labels for the AND gate\n",
        "inputs = np.array([[0, 0],\n",
        "                   [0, 1],\n",
        "                   [1, 0],\n",
        "                   [1, 1]])\n",
        "\n",
        "labels = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Define the weights and biases for the neural network\n",
        "weights = np.array([0.5, 0.5])\n",
        "bias = -0.7\n",
        "\n",
        "# Define the activation function (step function)\n",
        "def step_function(x):\n",
        "    return 0 if x <= 0 else 1\n",
        "\n",
        "# Perform the forward pass to compute the outputs\n",
        "outputs = []\n",
        "for input_row in inputs:\n",
        "    weighted_sum = np.dot(input_row, weights) + bias\n",
        "    output = step_function(weighted_sum)\n",
        "    outputs.append(output)\n",
        "\n",
        "# Compare the predicted outputs with the actual labels\n",
        "correct_predictions = np.sum(outputs == labels)\n",
        "\n",
        "# Print the results\n",
        "print(\"Inputs:\", inputs)\n",
        "print(\"Labels:\", labels)\n",
        "print(\"Predicted Outputs:\", outputs)\n",
        "print(\"Correct Predictions:\", correct_predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YVJKOvsWblb",
        "outputId": "cdda2f89-6345-4842-aeca-0ab952605cf5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: [[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "Labels: [0 0 0 1]\n",
            "Predicted Outputs: [0, 0, 0, 1]\n",
            "Correct Predictions: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xWp0bsHdX7Ft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise**: Design the previous examples of neural network by adjusting weights and biases **randomly**."
      ],
      "metadata": {
        "id": "uBJ1g-W_XTni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the inputs and corresponding labels for the AND gate\n",
        "inputs = np.array([[0, 0],\n",
        "                   [0, 1],\n",
        "                   [1, 0],\n",
        "                   [1, 1]])\n",
        "\n",
        "labels = np.array([0, 0, 0, 1])\n",
        "\n",
        "# Define the activation function (step function)\n",
        "def step_function(x):\n",
        "    return 0 if x <= 0 else 1\n",
        "\n",
        "# --- Modifying this part to generate random weights and bias ---\n",
        "# Randomly generate weights (2 weights for 2 inputs)\n",
        "weights = np.random.uniform(-1, 1, size=2)\n",
        "# Randomly generate a bias\n",
        "bias = np.random.uniform(-1, 1)\n",
        "\n",
        "print(\"Randomly initialized Weights:\", weights)\n",
        "print(\"Randomly initialized Bias:\", bias)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Perform the forward pass to compute the outputs\n",
        "outputs = []\n",
        "for input_row in inputs:\n",
        "    weighted_sum = np.dot(input_row, weights) + bias\n",
        "    output = step_function(weighted_sum)\n",
        "    outputs.append(output)\n",
        "\n",
        "# Compare the predicted outputs with the actual labels\n",
        "correct_predictions = np.sum(np.array(outputs) == labels)\n",
        "\n",
        "# Print the results\n",
        "print(\"Inputs:\", inputs)\n",
        "print(\"Labels:\", labels)\n",
        "print(\"Predicted Outputs:\", outputs)\n",
        "print(\"Correct Predictions:\", correct_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqWpWn0nWYyb",
        "outputId": "e24dd499-ac3f-4314-b08b-ddf9b044986e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Randomly initialized Weights: [-0.68091541  0.65915019]\n",
            "Randomly initialized Bias: 0.9790276273110528\n",
            "\n",
            "\n",
            "Inputs: [[0 0]\n",
            " [0 1]\n",
            " [1 0]\n",
            " [1 1]]\n",
            "Labels: [0 0 0 1]\n",
            "Predicted Outputs: [1, 1, 1, 1]\n",
            "Correct Predictions: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Exercise**: Design a neural network that has four input neurons, two hidden layers, and one output layer neuron. The activation function in the output payer should be sigmoidal. You should also adjust the weights and biases randomly. Moreover, the input dataset should contain 10 samples. Follow the figure: ![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "5P-IKE54X8sV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3135f2f6",
        "outputId": "7b760cae-adaa-4c4b-bb81-20c4fef688f7"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# --- 1. Define Network Architecture Parameters ---\n",
        "input_neurons = 4\n",
        "hidden_neurons_1 = 4 # From the figure\n",
        "hidden_neurons_2 = 3 # From the figure\n",
        "output_neurons = 1\n",
        "num_samples = 10\n",
        "\n",
        "# --- 2. Generate Random Input Dataset (10 samples, 4 features each) ---\n",
        "# Using np.random.rand for values between 0 and 1, simulating feature data\n",
        "inputs = np.random.rand(num_samples, input_neurons)\n",
        "print(f\"Input dataset shape: {inputs.shape}\\n\")\n",
        "\n",
        "# --- 3. Initialize Weights and Biases Randomly ---\n",
        "# Weights from Input Layer to Hidden Layer 1\n",
        "weights_input_h1 = np.random.randn(input_neurons, hidden_neurons_1)\n",
        "bias_h1 = np.random.randn(hidden_neurons_1)\n",
        "\n",
        "# Weights from Hidden Layer 1 to Hidden Layer 2\n",
        "weights_h1_h2 = np.random.randn(hidden_neurons_1, hidden_neurons_2)\n",
        "bias_h2 = np.random.randn(hidden_neurons_2)\n",
        "\n",
        "# Weights from Hidden Layer 2 to Output Layer\n",
        "weights_h2_output = np.random.randn(hidden_neurons_2, output_neurons)\n",
        "bias_output = np.random.randn(output_neurons)\n",
        "\n",
        "print(\"Randomly initialized weights and biases:\")\n",
        "print(f\"  Input to H1 Weights shape: {weights_input_h1.shape}\")\n",
        "print(f\"  H1 Biases shape: {bias_h1.shape}\")\n",
        "print(f\"  H1 to H2 Weights shape: {weights_h1_h2.shape}\")\n",
        "print(f\"  H2 Biases shape: {bias_h2.shape}\")\n",
        "print(f\"  H2 to Output Weights shape: {weights_h2_output.shape}\")\n",
        "print(f\"  Output Biases shape: {bias_output.shape}\\n\")\n",
        "\n",
        "# --- 4. Define Activation Functions ---\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# --- 5. Implement Forward Pass ---\n",
        "\n",
        "# Hidden Layer 1\n",
        "# Multiply inputs by weights, add bias, then apply ReLU activation\n",
        "hidden_layer_1_input = np.dot(inputs, weights_input_h1) + bias_h1\n",
        "hidden_layer_1_output = relu(hidden_layer_1_input)\n",
        "print(f\"Hidden Layer 1 Output shape: {hidden_layer_1_output.shape}\\n\")\n",
        "\n",
        "# Hidden Layer 2\n",
        "# Multiply Hidden Layer 1 output by weights, add bias, then apply ReLU activation\n",
        "hidden_layer_2_input = np.dot(hidden_layer_1_output, weights_h1_h2) + bias_h2\n",
        "hidden_layer_2_output = relu(hidden_layer_2_input)\n",
        "print(f\"Hidden Layer 2 Output shape: {hidden_layer_2_output.shape}\\n\")\n",
        "\n",
        "# Output Layer\n",
        "# Multiply Hidden Layer 2 output by weights, add bias, then apply Sigmoid activation\n",
        "output_layer_input = np.dot(hidden_layer_2_output, weights_h2_output) + bias_output\n",
        "final_output = sigmoid(output_layer_input)\n",
        "\n",
        "# --- 6. Print the Results ---\n",
        "print(f\"Final output of the neural network (shape: {final_output.shape}):\\n\", final_output)\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input dataset shape: (10, 4)\n",
            "\n",
            "Randomly initialized weights and biases:\n",
            "  Input to H1 Weights shape: (4, 4)\n",
            "  H1 Biases shape: (4,)\n",
            "  H1 to H2 Weights shape: (4, 3)\n",
            "  H2 Biases shape: (3,)\n",
            "  H2 to Output Weights shape: (3, 1)\n",
            "  Output Biases shape: (1,)\n",
            "\n",
            "Hidden Layer 1 Output shape: (10, 4)\n",
            "\n",
            "Hidden Layer 2 Output shape: (10, 3)\n",
            "\n",
            "Final output of the neural network (shape: (10, 1)):\n",
            " [[0.33924288]\n",
            " [0.33924288]\n",
            " [0.33924288]\n",
            " [0.33924288]\n",
            " [0.33924288]\n",
            " [0.33924288]\n",
            " [0.33924288]\n",
            " [0.33924288]\n",
            " [0.33924288]\n",
            " [0.33924288]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example-9: Neural network with loss calculation"
      ],
      "metadata": {
        "id": "0Xq8DCcAYunL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solve the and gate problem and then calculate the loss uisng mean square error.\n",
        "# run several times of the code and compare the error. Is the result same in every case? If so, why?\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# use sigmoidal activation\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define the neural network architecture\n",
        "input_size = 2\n",
        "hidden_size = 3\n",
        "output_size = 1\n",
        "\n",
        "# Initialize the weights and biases\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.random.randn(hidden_size)\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.random.randn(output_size)\n",
        "\n",
        "# Define the input for the AND gate\n",
        "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "\n",
        "# Define the target output for the AND gate\n",
        "target = np.array([[0], [0], [0], [1]])\n",
        "\n",
        "# Forward pass\n",
        "hidden_layer_output = (np.dot(x, W1) + b1)\n",
        "output = sigmoid(np.dot(hidden_layer_output, W2) + b2)\n",
        "\n",
        "# Calculate the loss (using Mean Squared Error)\n",
        "loss = np.mean((output - target) ** 2)\n",
        "\n",
        "print(\"Output:\")\n",
        "print(output)\n",
        "print(\"Loss:\", loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wevj4GKbXApn",
        "outputId": "ccffc991-0516-4849-8f90-f35cb19ec7e2"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output:\n",
            "[[0.742746  ]\n",
            " [0.73711575]\n",
            " [0.90618906]\n",
            " [0.90367199]]\n",
            "Loss: 0.48136723486432875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example-10: Calculate loss function and then minimize the error after Nth iterations by adjusting the parameters."
      ],
      "metadata": {
        "id": "CXlkRtNHY3n8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Use sigmoidal activation\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Creating loss function\n",
        "def loss(predicted, actual):\n",
        "    return np.mean((predicted - actual) ** 2)\n",
        "\n",
        "# Define the neural network architecture\n",
        "input_size = 2\n",
        "hidden_size = 3\n",
        "output_size = 1\n",
        "\n",
        "# Initialize the weights and biases\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.random.randn(hidden_size)\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.random.randn(output_size)\n",
        "\n",
        "# Define the input for the AND gate\n",
        "x = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "\n",
        "# Define the target output for the AND gate\n",
        "target = np.array([[0], [0], [0], [1]])\n",
        "\n",
        "# final_output\n",
        "final_output=None\n",
        "\n",
        "# Create some variables to track the best loss and the associated weights and biases\n",
        "best_loss = float('inf')\n",
        "best_W1 = None\n",
        "best_b1 = None\n",
        "best_W2 = None\n",
        "best_b2 = None\n",
        "\n",
        "for iteration in range(150):\n",
        "    # Forward pass\n",
        "    hidden_layer_output = np.dot(x, W1) + b1\n",
        "    output = sigmoid(np.dot(hidden_layer_output, W2) + b2)\n",
        "\n",
        "    # save final output\n",
        "    final_output=output\n",
        "\n",
        "    # Loss calculation\n",
        "    current_loss = loss(output, target)\n",
        "\n",
        "    if current_loss < best_loss:\n",
        "        print('New weights found, iteration:', iteration, 'loss:', current_loss)\n",
        "        # Save current weights and biases as the best ones so far\n",
        "        best_loss = current_loss\n",
        "        best_W1 = W1.copy()\n",
        "        best_b1 = b1.copy()\n",
        "        best_W2 = W2.copy()\n",
        "        best_b2 = b2.copy()\n",
        "    else:\n",
        "        # Revert weights and biases to the previous best values\n",
        "        W1 = best_W1.copy()\n",
        "        b1 = best_b1.copy()\n",
        "        W2 = best_W2.copy()\n",
        "        b2 = best_b2.copy()\n",
        "\n",
        "    # Update weights with small random values\n",
        "    W1 += 0.09 * np.random.randn(input_size, hidden_size)\n",
        "    b1 += 0.09 * np.random.randn(hidden_size)\n",
        "    W2 += 0.09 * np.random.randn(hidden_size, output_size)\n",
        "    b2 += 0.09 * np.random.randn(output_size)\n",
        "\n",
        "print(final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKVdmaWpY8ps",
        "outputId": "7cf76b68-88bf-487d-e574-e745b96d2d93"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New weights found, iteration: 0 loss: 0.2347731463732542\n",
            "New weights found, iteration: 3 loss: 0.22031034204068545\n",
            "New weights found, iteration: 4 loss: 0.2086685818384743\n",
            "New weights found, iteration: 5 loss: 0.2074900336832569\n",
            "New weights found, iteration: 7 loss: 0.1984493834414801\n",
            "New weights found, iteration: 9 loss: 0.19283991031426514\n",
            "New weights found, iteration: 10 loss: 0.1844685798525191\n",
            "New weights found, iteration: 11 loss: 0.16248243404076773\n",
            "New weights found, iteration: 12 loss: 0.15669473179373553\n",
            "New weights found, iteration: 16 loss: 0.1411247504231922\n",
            "New weights found, iteration: 17 loss: 0.11947925544675897\n",
            "New weights found, iteration: 18 loss: 0.10775747528377239\n",
            "New weights found, iteration: 20 loss: 0.0947724919423292\n",
            "New weights found, iteration: 22 loss: 0.09369192550827685\n",
            "New weights found, iteration: 23 loss: 0.08670190436168047\n",
            "New weights found, iteration: 27 loss: 0.07241650170231345\n",
            "New weights found, iteration: 30 loss: 0.06332799121698064\n",
            "New weights found, iteration: 32 loss: 0.06115605414111089\n",
            "New weights found, iteration: 33 loss: 0.056737586351120195\n",
            "New weights found, iteration: 37 loss: 0.04740534301901572\n",
            "New weights found, iteration: 39 loss: 0.04371910217096789\n",
            "New weights found, iteration: 42 loss: 0.03316315079846813\n",
            "New weights found, iteration: 47 loss: 0.03220536682999442\n",
            "New weights found, iteration: 49 loss: 0.030783461394838688\n",
            "New weights found, iteration: 57 loss: 0.030280601161855096\n",
            "New weights found, iteration: 58 loss: 0.02982508542594799\n",
            "New weights found, iteration: 59 loss: 0.027410258602678157\n",
            "New weights found, iteration: 61 loss: 0.02567305812639418\n",
            "New weights found, iteration: 67 loss: 0.024087056274212024\n",
            "New weights found, iteration: 71 loss: 0.017915824157707302\n",
            "New weights found, iteration: 73 loss: 0.015769325012801146\n",
            "New weights found, iteration: 75 loss: 0.015650154958549496\n",
            "New weights found, iteration: 76 loss: 0.013103509156309319\n",
            "New weights found, iteration: 81 loss: 0.012561342875973849\n",
            "New weights found, iteration: 83 loss: 0.011872037575168838\n",
            "New weights found, iteration: 95 loss: 0.010986773007383077\n",
            "New weights found, iteration: 107 loss: 0.010739217721695246\n",
            "New weights found, iteration: 108 loss: 0.00943699939736502\n",
            "New weights found, iteration: 109 loss: 0.008519119197487188\n",
            "New weights found, iteration: 110 loss: 0.006885384381071031\n",
            "New weights found, iteration: 114 loss: 0.006628331863818657\n",
            "New weights found, iteration: 115 loss: 0.004159558866431423\n",
            "New weights found, iteration: 119 loss: 0.003824316991790546\n",
            "New weights found, iteration: 122 loss: 0.00252883651944871\n",
            "New weights found, iteration: 124 loss: 0.0025053796410316968\n",
            "New weights found, iteration: 136 loss: 0.0022903175655933257\n",
            "New weights found, iteration: 141 loss: 0.0018354249320890908\n",
            "New weights found, iteration: 147 loss: 0.0017998935966737153\n",
            "[[1.56944548e-04]\n",
            " [3.14730429e-02]\n",
            " [5.60043967e-02]\n",
            " [9.24709441e-01]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example-11: Solving AND gate problem using neural network with backpropagation approach."
      ],
      "metadata": {
        "id": "Uthey0CtZE2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define the derivative of the sigmoid activation function\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Define the input data and labels for the AND gate problem\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [0], [0], [1]])\n",
        "\n",
        "# Initialize the weights and biases with random values\n",
        "np.random.seed(42)\n",
        "W = np.random.randn(2, 1)\n",
        "b = np.random.randn(1)\n",
        "\n",
        "# Set the learning rate and number of epochs\n",
        "learning_rate = 0.8\n",
        "num_epochs = 500\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    z = np.dot(X, W) + b\n",
        "    a = sigmoid(z)\n",
        "\n",
        "    # Compute the loss\n",
        "    loss = np.mean((y - a) ** 2)\n",
        "\n",
        "    error= (y - a)\n",
        "    # Backpropagation\n",
        "    delta = error * sigmoid_derivative(z)\n",
        "    dW = np.dot(X.T, delta)\n",
        "    db = np.sum(delta)\n",
        "\n",
        "    # Update the weights and biases\n",
        "    W -= learning_rate * dW\n",
        "    b -= learning_rate * db\n",
        "\n",
        "    # Print the loss at every 100 epochs\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f\"Epoch {epoch + 1}: Loss = {loss}\")\n",
        "\n",
        "# Test the model on the input data\n",
        "#predictions = (a >= 0.5).astype(int)\n",
        "print(\"Predicted Output:\")\n",
        "print(a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpfX-LWYZElu",
        "outputId": "3b5a3236-8a17-458e-ed42-d1adcea5bb13"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 100: Loss = 0.743998459915124\n",
            "Epoch 200: Loss = 0.7470214794177472\n",
            "Epoch 300: Loss = 0.7480165507492393\n",
            "Epoch 400: Loss = 0.748512245347843\n",
            "Epoch 500: Loss = 0.7488092427954165\n",
            "Predicted Output:\n",
            "[[0.99836331]\n",
            " [0.99958641]\n",
            " [0.99966728]\n",
            " [0.999916  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12 . a simple neural network with 2 inputs, 2 hidden neurons, and 1 output neuron. The network is trained using the backpropagation algorithm, with sigmoid activation and mean squared error (MSE) loss. It uses gradient descent to update weights and biases to minimize the loss."
      ],
      "metadata": {
        "id": "1q44hUzNn5hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# source doce: https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "  # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def deriv_sigmoid(x):\n",
        "  # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
        "  fx = sigmoid(x)\n",
        "  return fx * (1 - fx)\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "  # y_true and y_pred are numpy arrays of the same length.\n",
        "  return ((y_true - y_pred) ** 2).mean()\n",
        "\n",
        "class OurNeuralNetwork:\n",
        "  '''\n",
        "  A neural network with:\n",
        "    - 2 inputs\n",
        "    - a hidden layer with 2 neurons (h1, h2)\n",
        "    - an output layer with 1 neuron (o1)\n",
        "\n",
        "  *** DISCLAIMER ***:\n",
        "  The code below is intended to be simple and educational, NOT optimal.\n",
        "  Real neural net code looks nothing like this. DO NOT use this code.\n",
        "  Instead, read/run it to understand how this specific network works.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    # Weights\n",
        "    self.w1 = np.random.normal()\n",
        "    self.w2 = np.random.normal()\n",
        "    self.w3 = np.random.normal()\n",
        "    self.w4 = np.random.normal()\n",
        "    self.w5 = np.random.normal()\n",
        "    self.w6 = np.random.normal()\n",
        "\n",
        "    # Biases\n",
        "    self.b1 = np.random.normal()\n",
        "    self.b2 = np.random.normal()\n",
        "    self.b3 = np.random.normal()\n",
        "\n",
        "  def feedforward(self, x):\n",
        "    # x is a numpy array with 2 elements.\n",
        "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
        "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
        "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
        "    return o1\n",
        "\n",
        "  def train(self, data, all_y_trues):\n",
        "    '''\n",
        "    - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
        "    - all_y_trues is a numpy array with n elements.\n",
        "      Elements in all_y_trues correspond to those in data.\n",
        "    '''\n",
        "    learn_rate = 0.1\n",
        "    epochs = 1000 # number of times to loop through the entire dataset\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      for x, y_true in zip(data, all_y_trues):\n",
        "        # --- Do a feedforward (we'll need these values later)\n",
        "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
        "        h1 = sigmoid(sum_h1)\n",
        "\n",
        "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
        "        h2 = sigmoid(sum_h2)\n",
        "\n",
        "        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
        "        o1 = sigmoid(sum_o1)\n",
        "        y_pred = o1\n",
        "\n",
        "        # --- Calculate partial derivatives.\n",
        "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
        "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
        "\n",
        "        # Neuron o1\n",
        "        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
        "\n",
        "        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
        "\n",
        "        # Neuron h1\n",
        "        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
        "        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
        "        d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
        "\n",
        "        # Neuron h2\n",
        "        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
        "        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
        "        d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
        "\n",
        "        # --- Update weights and biases\n",
        "        # Neuron h1\n",
        "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
        "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
        "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
        "\n",
        "        # Neuron h2\n",
        "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
        "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
        "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
        "\n",
        "        # Neuron o1\n",
        "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
        "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
        "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
        "\n",
        "      # --- Calculate total loss at the end of each epoch\n",
        "      if epoch % 10 == 0:\n",
        "        y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
        "        loss = mse_loss(all_y_trues, y_preds)\n",
        "        print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
        "\n",
        "\n",
        "# Define dataset\n",
        "data = np.array([\n",
        "  [-2, -1],  # Alice\n",
        "  [25, 6],   # Bob\n",
        "  [17, 4],   # Charlie\n",
        "  [-15, -6], # Diana\n",
        "])\n",
        "all_y_trues = np.array([\n",
        "  1, # Alice\n",
        "  0, # Bob\n",
        "  0, # Charlie\n",
        "  1, # Diana\n",
        "])\n",
        "\n",
        "# Train our neural network!\n",
        "network = OurNeuralNetwork()\n",
        "network.train(data, all_y_trues)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NduXMlX4Y9c0",
        "outputId": "2902f3da-01db-4e41-985e-5e4d7a4f6d31"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 0.288\n",
            "Epoch 10 loss: 0.218\n",
            "Epoch 20 loss: 0.170\n",
            "Epoch 30 loss: 0.134\n",
            "Epoch 40 loss: 0.107\n",
            "Epoch 50 loss: 0.087\n",
            "Epoch 60 loss: 0.072\n",
            "Epoch 70 loss: 0.060\n",
            "Epoch 80 loss: 0.052\n",
            "Epoch 90 loss: 0.045\n",
            "Epoch 100 loss: 0.039\n",
            "Epoch 110 loss: 0.035\n",
            "Epoch 120 loss: 0.031\n",
            "Epoch 130 loss: 0.028\n",
            "Epoch 140 loss: 0.026\n",
            "Epoch 150 loss: 0.024\n",
            "Epoch 160 loss: 0.022\n",
            "Epoch 170 loss: 0.020\n",
            "Epoch 180 loss: 0.019\n",
            "Epoch 190 loss: 0.017\n",
            "Epoch 200 loss: 0.016\n",
            "Epoch 210 loss: 0.015\n",
            "Epoch 220 loss: 0.015\n",
            "Epoch 230 loss: 0.014\n",
            "Epoch 240 loss: 0.013\n",
            "Epoch 250 loss: 0.012\n",
            "Epoch 260 loss: 0.012\n",
            "Epoch 270 loss: 0.011\n",
            "Epoch 280 loss: 0.011\n",
            "Epoch 290 loss: 0.010\n",
            "Epoch 300 loss: 0.010\n",
            "Epoch 310 loss: 0.009\n",
            "Epoch 320 loss: 0.009\n",
            "Epoch 330 loss: 0.009\n",
            "Epoch 340 loss: 0.008\n",
            "Epoch 350 loss: 0.008\n",
            "Epoch 360 loss: 0.008\n",
            "Epoch 370 loss: 0.008\n",
            "Epoch 380 loss: 0.007\n",
            "Epoch 390 loss: 0.007\n",
            "Epoch 400 loss: 0.007\n",
            "Epoch 410 loss: 0.007\n",
            "Epoch 420 loss: 0.007\n",
            "Epoch 430 loss: 0.006\n",
            "Epoch 440 loss: 0.006\n",
            "Epoch 450 loss: 0.006\n",
            "Epoch 460 loss: 0.006\n",
            "Epoch 470 loss: 0.006\n",
            "Epoch 480 loss: 0.006\n",
            "Epoch 490 loss: 0.005\n",
            "Epoch 500 loss: 0.005\n",
            "Epoch 510 loss: 0.005\n",
            "Epoch 520 loss: 0.005\n",
            "Epoch 530 loss: 0.005\n",
            "Epoch 540 loss: 0.005\n",
            "Epoch 550 loss: 0.005\n",
            "Epoch 560 loss: 0.005\n",
            "Epoch 570 loss: 0.005\n",
            "Epoch 580 loss: 0.004\n",
            "Epoch 590 loss: 0.004\n",
            "Epoch 600 loss: 0.004\n",
            "Epoch 610 loss: 0.004\n",
            "Epoch 620 loss: 0.004\n",
            "Epoch 630 loss: 0.004\n",
            "Epoch 640 loss: 0.004\n",
            "Epoch 650 loss: 0.004\n",
            "Epoch 660 loss: 0.004\n",
            "Epoch 670 loss: 0.004\n",
            "Epoch 680 loss: 0.004\n",
            "Epoch 690 loss: 0.004\n",
            "Epoch 700 loss: 0.004\n",
            "Epoch 710 loss: 0.004\n",
            "Epoch 720 loss: 0.003\n",
            "Epoch 730 loss: 0.003\n",
            "Epoch 740 loss: 0.003\n",
            "Epoch 750 loss: 0.003\n",
            "Epoch 760 loss: 0.003\n",
            "Epoch 770 loss: 0.003\n",
            "Epoch 780 loss: 0.003\n",
            "Epoch 790 loss: 0.003\n",
            "Epoch 800 loss: 0.003\n",
            "Epoch 810 loss: 0.003\n",
            "Epoch 820 loss: 0.003\n",
            "Epoch 830 loss: 0.003\n",
            "Epoch 840 loss: 0.003\n",
            "Epoch 850 loss: 0.003\n",
            "Epoch 860 loss: 0.003\n",
            "Epoch 870 loss: 0.003\n",
            "Epoch 880 loss: 0.003\n",
            "Epoch 890 loss: 0.003\n",
            "Epoch 900 loss: 0.003\n",
            "Epoch 910 loss: 0.003\n",
            "Epoch 920 loss: 0.003\n",
            "Epoch 930 loss: 0.003\n",
            "Epoch 940 loss: 0.003\n",
            "Epoch 950 loss: 0.003\n",
            "Epoch 960 loss: 0.002\n",
            "Epoch 970 loss: 0.002\n",
            "Epoch 980 loss: 0.002\n",
            "Epoch 990 loss: 0.002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise: draw the following figure based on the outputs of example-11:"
      ],
      "metadata": {
        "id": "vkHXGr2aoe4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def deriv_sigmoid(x):\n",
        "    fx = sigmoid(x)\n",
        "    return fx * (1 - fx)\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "    return ((y_true - y_pred) ** 2).mean()\n",
        "\n",
        "class OurNeuralNetwork:\n",
        "    def __init__(self):\n",
        "        # Initialize weights and biases randomly\n",
        "        self.w1 = np.random.normal()\n",
        "        self.w2 = np.random.normal()\n",
        "        self.w3 = np.random.normal()\n",
        "        self.w4 = np.random.normal()\n",
        "        self.w5 = np.random.normal()\n",
        "        self.w6 = np.random.normal()\n",
        "\n",
        "        self.b1 = np.random.normal()\n",
        "        self.b2 = np.random.normal()\n",
        "        self.b3 = np.random.normal()\n",
        "\n",
        "    def feedforward(self, x):\n",
        "        h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
        "        h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
        "        o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
        "        return o1\n",
        "\n",
        "    def train(self, data, all_y_trues):\n",
        "        learn_rate = 0.1\n",
        "        epochs = 1000  # Number of iterations\n",
        "\n",
        "        # List to store the loss values for each epoch\n",
        "        loss_history = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            for x, y_true in zip(data, all_y_trues):\n",
        "                # Feedforward\n",
        "                sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
        "                h1 = sigmoid(sum_h1)\n",
        "\n",
        "                sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
        "                h2 = sigmoid(sum_h2)\n",
        "\n",
        "                sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
        "                o1 = sigmoid(sum_o1)\n",
        "                y_pred = o1\n",
        "\n",
        "                # Backpropagation (calculating gradients)\n",
        "                d_L_d_ypred = -2 * (y_true - y_pred)\n",
        "                d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
        "                d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
        "                d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
        "\n",
        "                d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
        "                d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
        "\n",
        "                d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
        "                d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
        "                d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
        "\n",
        "                d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
        "                d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
        "                d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
        "\n",
        "                # Update weights and biases using gradient descent\n",
        "                self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
        "                self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
        "                self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
        "\n",
        "                self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
        "                self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
        "                self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
        "\n",
        "                self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
        "                self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
        "                self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
        "\n",
        "            # Track the loss every 10 epochs\n",
        "            if epoch % 10 == 0:\n",
        "                y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
        "                loss = mse_loss(all_y_trues, y_preds)\n",
        "                loss_history.append(loss)\n",
        "                print(f\"Epoch {epoch} loss: {loss:.3f}\")\n",
        "\n",
        "        # After training, plot the loss curve\n",
        "        plt.plot(loss_history)\n",
        "        plt.title('Loss over Epochs')\n",
        "        plt.xlabel('Epochs (x10)')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.show()\n",
        "\n",
        "# Define dataset\n",
        "data = np.array([\n",
        "  [-2, -1],  # Alice\n",
        "  [25, 6],   # Bob\n",
        "  [17, 4],   # Charlie\n",
        "  [-15, -6], # Diana\n",
        "])\n",
        "\n",
        "all_y_trues = np.array([\n",
        "  1, # Alice\n",
        "  0, # Bob\n",
        "  0, # Charlie\n",
        "  1, # Diana\n",
        "])\n",
        "\n",
        "# Train and plot the neural network\n",
        "network = OurNeuralNetwork()\n",
        "network.train(data, all_y_trues)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FKgGOKzMn-xZ",
        "outputId": "f1db101b-49f0-4755-9fed-d6d779878d3b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 0.365\n",
            "Epoch 10 loss: 0.150\n",
            "Epoch 20 loss: 0.086\n",
            "Epoch 30 loss: 0.061\n",
            "Epoch 40 loss: 0.048\n",
            "Epoch 50 loss: 0.039\n",
            "Epoch 60 loss: 0.032\n",
            "Epoch 70 loss: 0.027\n",
            "Epoch 80 loss: 0.024\n",
            "Epoch 90 loss: 0.021\n",
            "Epoch 100 loss: 0.019\n",
            "Epoch 110 loss: 0.017\n",
            "Epoch 120 loss: 0.015\n",
            "Epoch 130 loss: 0.014\n",
            "Epoch 140 loss: 0.013\n",
            "Epoch 150 loss: 0.012\n",
            "Epoch 160 loss: 0.011\n",
            "Epoch 170 loss: 0.010\n",
            "Epoch 180 loss: 0.010\n",
            "Epoch 190 loss: 0.009\n",
            "Epoch 200 loss: 0.009\n",
            "Epoch 210 loss: 0.008\n",
            "Epoch 220 loss: 0.008\n",
            "Epoch 230 loss: 0.007\n",
            "Epoch 240 loss: 0.007\n",
            "Epoch 250 loss: 0.007\n",
            "Epoch 260 loss: 0.006\n",
            "Epoch 270 loss: 0.006\n",
            "Epoch 280 loss: 0.006\n",
            "Epoch 290 loss: 0.006\n",
            "Epoch 300 loss: 0.005\n",
            "Epoch 310 loss: 0.005\n",
            "Epoch 320 loss: 0.005\n",
            "Epoch 330 loss: 0.005\n",
            "Epoch 340 loss: 0.005\n",
            "Epoch 350 loss: 0.005\n",
            "Epoch 360 loss: 0.004\n",
            "Epoch 370 loss: 0.004\n",
            "Epoch 380 loss: 0.004\n",
            "Epoch 390 loss: 0.004\n",
            "Epoch 400 loss: 0.004\n",
            "Epoch 410 loss: 0.004\n",
            "Epoch 420 loss: 0.004\n",
            "Epoch 430 loss: 0.004\n",
            "Epoch 440 loss: 0.004\n",
            "Epoch 450 loss: 0.004\n",
            "Epoch 460 loss: 0.003\n",
            "Epoch 470 loss: 0.003\n",
            "Epoch 480 loss: 0.003\n",
            "Epoch 490 loss: 0.003\n",
            "Epoch 500 loss: 0.003\n",
            "Epoch 510 loss: 0.003\n",
            "Epoch 520 loss: 0.003\n",
            "Epoch 530 loss: 0.003\n",
            "Epoch 540 loss: 0.003\n",
            "Epoch 550 loss: 0.003\n",
            "Epoch 560 loss: 0.003\n",
            "Epoch 570 loss: 0.003\n",
            "Epoch 580 loss: 0.003\n",
            "Epoch 590 loss: 0.003\n",
            "Epoch 600 loss: 0.003\n",
            "Epoch 610 loss: 0.003\n",
            "Epoch 620 loss: 0.002\n",
            "Epoch 630 loss: 0.002\n",
            "Epoch 640 loss: 0.002\n",
            "Epoch 650 loss: 0.002\n",
            "Epoch 660 loss: 0.002\n",
            "Epoch 670 loss: 0.002\n",
            "Epoch 680 loss: 0.002\n",
            "Epoch 690 loss: 0.002\n",
            "Epoch 700 loss: 0.002\n",
            "Epoch 710 loss: 0.002\n",
            "Epoch 720 loss: 0.002\n",
            "Epoch 730 loss: 0.002\n",
            "Epoch 740 loss: 0.002\n",
            "Epoch 750 loss: 0.002\n",
            "Epoch 760 loss: 0.002\n",
            "Epoch 770 loss: 0.002\n",
            "Epoch 780 loss: 0.002\n",
            "Epoch 790 loss: 0.002\n",
            "Epoch 800 loss: 0.002\n",
            "Epoch 810 loss: 0.002\n",
            "Epoch 820 loss: 0.002\n",
            "Epoch 830 loss: 0.002\n",
            "Epoch 840 loss: 0.002\n",
            "Epoch 850 loss: 0.002\n",
            "Epoch 860 loss: 0.002\n",
            "Epoch 870 loss: 0.002\n",
            "Epoch 880 loss: 0.002\n",
            "Epoch 890 loss: 0.002\n",
            "Epoch 900 loss: 0.002\n",
            "Epoch 910 loss: 0.002\n",
            "Epoch 920 loss: 0.002\n",
            "Epoch 930 loss: 0.002\n",
            "Epoch 940 loss: 0.002\n",
            "Epoch 950 loss: 0.002\n",
            "Epoch 960 loss: 0.002\n",
            "Epoch 970 loss: 0.002\n",
            "Epoch 980 loss: 0.001\n",
            "Epoch 990 loss: 0.001\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# source doce: https://towardsdatascience.com/machine-learning-for-beginners-an-introduction-to-neural-networks-d49f22d238f9\n",
        "\n",
        "import numpy as np\n",
        "rafi=[]\n",
        "shafi=[]\n",
        "def sigmoid(x):\n",
        "  # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def deriv_sigmoid(x):\n",
        "  # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
        "  fx = sigmoid(x)\n",
        "  return fx * (1 - fx)\n",
        "\n",
        "def mse_loss(y_true, y_pred):\n",
        "  # y_true and y_pred are numpy arrays of the same length.\n",
        "  return ((y_true - y_pred) ** 2).mean()\n",
        "\n",
        "class OurNeuralNetwork:\n",
        "  '''\n",
        "  A neural network with:\n",
        "    - 2 inputs\n",
        "    - a hidden layer with 2 neurons (h1, h2)\n",
        "    - an output layer with 1 neuron (o1)\n",
        "\n",
        "  *** DISCLAIMER ***:\n",
        "  The code below is intended to be simple and educational, NOT optimal.\n",
        "  Real neural net code looks nothing like this. DO NOT use this code.\n",
        "  Instead, read/run it to understand how this specific network works.\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    # Weights\n",
        "    self.w1 = np.random.normal()\n",
        "    self.w2 = np.random.normal()\n",
        "    self.w3 = np.random.normal()\n",
        "    self.w4 = np.random.normal()\n",
        "    self.w5 = np.random.normal()\n",
        "    self.w6 = np.random.normal()\n",
        "\n",
        "    # Biases\n",
        "    self.b1 = np.random.normal()\n",
        "    self.b2 = np.random.normal()\n",
        "    self.b3 = np.random.normal()\n",
        "\n",
        "  def feedforward(self, x):\n",
        "    # x is a numpy array with 2 elements.\n",
        "    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)\n",
        "    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)\n",
        "    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)\n",
        "    return o1\n",
        "\n",
        "  def train(self, data, all_y_trues):\n",
        "    '''\n",
        "    - data is a (n x 2) numpy array, n = # of samples in the dataset.\n",
        "    - all_y_trues is a numpy array with n elements.\n",
        "      Elements in all_y_trues correspond to those in data.\n",
        "    '''\n",
        "    learn_rate = 0.1\n",
        "    epochs = 1000 # number of times to loop through the entire dataset\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      for x, y_true in zip(data, all_y_trues):\n",
        "        # --- Do a feedforward (we'll need these values later)\n",
        "        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1\n",
        "        h1 = sigmoid(sum_h1)\n",
        "\n",
        "        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2\n",
        "        h2 = sigmoid(sum_h2)\n",
        "\n",
        "        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3\n",
        "        o1 = sigmoid(sum_o1)\n",
        "        y_pred = o1\n",
        "\n",
        "        # --- Calculate partial derivatives.\n",
        "        # --- Naming: d_L_d_w1 represents \"partial L / partial w1\"\n",
        "        d_L_d_ypred = -2 * (y_true - y_pred)\n",
        "\n",
        "        # Neuron o1\n",
        "        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_b3 = deriv_sigmoid(sum_o1)\n",
        "\n",
        "        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)\n",
        "        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)\n",
        "\n",
        "        # Neuron h1\n",
        "        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)\n",
        "        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)\n",
        "        d_h1_d_b1 = deriv_sigmoid(sum_h1)\n",
        "\n",
        "        # Neuron h2\n",
        "        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)\n",
        "        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)\n",
        "        d_h2_d_b2 = deriv_sigmoid(sum_h2)\n",
        "\n",
        "        # --- Update weights and biases\n",
        "        # Neuron h1\n",
        "        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1\n",
        "        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2\n",
        "        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1\n",
        "\n",
        "        # Neuron h2\n",
        "        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3\n",
        "        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4\n",
        "        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2\n",
        "\n",
        "        # Neuron o1\n",
        "        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5\n",
        "        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6\n",
        "        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3\n",
        "\n",
        "      # --- Calculate total loss at the end of each epoch\n",
        "      if epoch % 10 == 0:\n",
        "        y_preds = np.apply_along_axis(self.feedforward, 1, data)\n",
        "        loss = mse_loss(all_y_trues, y_preds)\n",
        "        print(\"Epoch %d loss: %.3f\" % (epoch, loss))\n",
        "        rafi.append(epoch)\n",
        "        shafi.append(loss)\n",
        "\n",
        "# Define dataset\n",
        "data = np.array([\n",
        "  [-2, -1],  # Alice\n",
        "  [25, 6],   # Bob\n",
        "  [17, 4],   # Charlie\n",
        "  [-15, -6], # Diana\n",
        "])\n",
        "all_y_trues = np.array([\n",
        "  1, # Alice\n",
        "  0, # Bob\n",
        "  0, # Charlie\n",
        "  1, # Diana\n",
        "])\n",
        "\n",
        "# Train our neural network!\n",
        "network = OurNeuralNetwork()\n",
        "network.train(data, all_y_trues)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJ3eOKVVocMT",
        "outputId": "ab959c55-cdd6-4142-d7ff-1127ad50e862"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 loss: 0.391\n",
            "Epoch 10 loss: 0.249\n",
            "Epoch 20 loss: 0.168\n",
            "Epoch 30 loss: 0.127\n",
            "Epoch 40 loss: 0.100\n",
            "Epoch 50 loss: 0.081\n",
            "Epoch 60 loss: 0.067\n",
            "Epoch 70 loss: 0.057\n",
            "Epoch 80 loss: 0.048\n",
            "Epoch 90 loss: 0.042\n",
            "Epoch 100 loss: 0.037\n",
            "Epoch 110 loss: 0.033\n",
            "Epoch 120 loss: 0.030\n",
            "Epoch 130 loss: 0.027\n",
            "Epoch 140 loss: 0.024\n",
            "Epoch 150 loss: 0.022\n",
            "Epoch 160 loss: 0.021\n",
            "Epoch 170 loss: 0.019\n",
            "Epoch 180 loss: 0.018\n",
            "Epoch 190 loss: 0.017\n",
            "Epoch 200 loss: 0.016\n",
            "Epoch 210 loss: 0.015\n",
            "Epoch 220 loss: 0.014\n",
            "Epoch 230 loss: 0.013\n",
            "Epoch 240 loss: 0.013\n",
            "Epoch 250 loss: 0.012\n",
            "Epoch 260 loss: 0.011\n",
            "Epoch 270 loss: 0.011\n",
            "Epoch 280 loss: 0.010\n",
            "Epoch 290 loss: 0.010\n",
            "Epoch 300 loss: 0.010\n",
            "Epoch 310 loss: 0.009\n",
            "Epoch 320 loss: 0.009\n",
            "Epoch 330 loss: 0.009\n",
            "Epoch 340 loss: 0.008\n",
            "Epoch 350 loss: 0.008\n",
            "Epoch 360 loss: 0.008\n",
            "Epoch 370 loss: 0.007\n",
            "Epoch 380 loss: 0.007\n",
            "Epoch 390 loss: 0.007\n",
            "Epoch 400 loss: 0.007\n",
            "Epoch 410 loss: 0.007\n",
            "Epoch 420 loss: 0.006\n",
            "Epoch 430 loss: 0.006\n",
            "Epoch 440 loss: 0.006\n",
            "Epoch 450 loss: 0.006\n",
            "Epoch 460 loss: 0.006\n",
            "Epoch 470 loss: 0.006\n",
            "Epoch 480 loss: 0.005\n",
            "Epoch 490 loss: 0.005\n",
            "Epoch 500 loss: 0.005\n",
            "Epoch 510 loss: 0.005\n",
            "Epoch 520 loss: 0.005\n",
            "Epoch 530 loss: 0.005\n",
            "Epoch 540 loss: 0.005\n",
            "Epoch 550 loss: 0.005\n",
            "Epoch 560 loss: 0.005\n",
            "Epoch 570 loss: 0.004\n",
            "Epoch 580 loss: 0.004\n",
            "Epoch 590 loss: 0.004\n",
            "Epoch 600 loss: 0.004\n",
            "Epoch 610 loss: 0.004\n",
            "Epoch 620 loss: 0.004\n",
            "Epoch 630 loss: 0.004\n",
            "Epoch 640 loss: 0.004\n",
            "Epoch 650 loss: 0.004\n",
            "Epoch 660 loss: 0.004\n",
            "Epoch 670 loss: 0.004\n",
            "Epoch 680 loss: 0.004\n",
            "Epoch 690 loss: 0.004\n",
            "Epoch 700 loss: 0.004\n",
            "Epoch 710 loss: 0.003\n",
            "Epoch 720 loss: 0.003\n",
            "Epoch 730 loss: 0.003\n",
            "Epoch 740 loss: 0.003\n",
            "Epoch 750 loss: 0.003\n",
            "Epoch 760 loss: 0.003\n",
            "Epoch 770 loss: 0.003\n",
            "Epoch 780 loss: 0.003\n",
            "Epoch 790 loss: 0.003\n",
            "Epoch 800 loss: 0.003\n",
            "Epoch 810 loss: 0.003\n",
            "Epoch 820 loss: 0.003\n",
            "Epoch 830 loss: 0.003\n",
            "Epoch 840 loss: 0.003\n",
            "Epoch 850 loss: 0.003\n",
            "Epoch 860 loss: 0.003\n",
            "Epoch 870 loss: 0.003\n",
            "Epoch 880 loss: 0.003\n",
            "Epoch 890 loss: 0.003\n",
            "Epoch 900 loss: 0.003\n",
            "Epoch 910 loss: 0.003\n",
            "Epoch 920 loss: 0.003\n",
            "Epoch 930 loss: 0.003\n",
            "Epoch 940 loss: 0.003\n",
            "Epoch 950 loss: 0.002\n",
            "Epoch 960 loss: 0.002\n",
            "Epoch 970 loss: 0.002\n",
            "Epoch 980 loss: 0.002\n",
            "Epoch 990 loss: 0.002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tlrTlj4YosT1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}