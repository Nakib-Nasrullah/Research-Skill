{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7ZljcXwj8XdOuobUIStTq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nakib-Nasrullah/Research-Skill/blob/main/Exam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#a simple neural network with one output neuron, four inputs and weights, and **biase**"
      ],
      "metadata": {
        "id": "CJX6YtHy32lW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "urX7PxZa7HDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e928fd96-0adf-4f9d-f3a8-3284cbf743b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.8\n"
          ]
        }
      ],
      "source": [
        "#a simple neural network with one output neuron, four inputs and weights, and biase\n",
        "inputs = [1.0, 2.0, 3.0,2.5]\n",
        "weights = [0.2, 0.8, -0.5, 1.0]\n",
        "bias = 2.0\n",
        "output = (inputs[0]*weights[0]+inputs[1]*weights[1]+inputs[2]*weights[2]+inputs[3]*weights[3]+bias)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Design the previous examples of neural network by adjusting weights and biases randomly.\n"
      ],
      "metadata": {
        "id": "SHguIEqS4DdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Inputs\n",
        "inputs = [1.0, 2.0, 3.0, 2.5]\n",
        "\n",
        "# Randomly generate weights (one weight per input)\n",
        "weights = [random.uniform(-1, 1) for i in range(len(inputs))]\n",
        "\n",
        "# Randomly generate a bias\n",
        "bias = random.uniform(-1, 1)\n",
        "\n",
        "# Calculate output of the neuron\n",
        "output = sum(i*w for i, w in zip(inputs, weights)) + bias\n",
        "\n",
        "print(\"Inputs:\", inputs)\n",
        "print(\"Weights:\", weights)\n",
        "print(\"Bias:\", bias)\n",
        "print(\"Output:\", output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcjV6MWs3-OQ",
        "outputId": "8e49f50d-b60e-400c-c52b-84745fc57238"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs: [1.0, 2.0, 3.0, 2.5]\n",
            "Weights: [-0.10529846248416508, -0.4162741507858594, -0.17155353958573838, 0.3754115481168734]\n",
            "Bias: 0.35350245043591855\n",
            "Output: -0.16047606208499698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example-2: A fully connected neural network — every neuron in the current layer has connections to every neuron from the previous."
      ],
      "metadata": {
        "id": "OcMoO_B04jqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let’s say we have a scenario with 3 neurons in a layer and 4 inputs:\n",
        "#\n",
        "inputs = [1, 2, 3, 2.5]\n",
        "weights1 = [0.2, 0.8, -0.5, 1]\n",
        "weights2 = [0.5, -0.91, 0.26, -0.5]\n",
        "weights3 = [-0.26, -0.27, 0.17, 0.87]\n",
        "bias1 = 2\n",
        "bias2 = 3\n",
        "bias3 =0.5\n",
        "\n",
        "outputs = [\n",
        "    # Neuron 1:\n",
        "    inputs[0]*weights1[0]+\n",
        "    inputs[1]*weights1[1]+\n",
        "    inputs[2]*weights1[2]+\n",
        "    inputs[3]*weights1[3]+bias1,\n",
        "\n",
        "    # Neuron 2:\n",
        "    inputs[0]*weights2[0]+\n",
        "    inputs[1]*weights2[1]+\n",
        "    inputs[2]*weights2[2]+\n",
        "    inputs[3]*weights2[3]+bias2,\n",
        "\n",
        "    # Neuron 3:\n",
        "    inputs[0]*weights3[0]+\n",
        "    inputs[1]*weights3[1]+\n",
        "    inputs[2]*weights3[2]+\n",
        "    inputs[3]*weights3[3]+bias3]\n",
        "\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osmCuEFU4ZLz",
        "outputId": "3820d10a-038d-4a4d-ec42-452986a061d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.8, 1.21, 2.385]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Exercise**: A fully connected neural network — every neuron in the current layer has connections to every neuron from the previous layer. You have 4 input neurons, one hidden layer consisting of 3 neurons and one output neuron. You should adust weights and biases randomly."
      ],
      "metadata": {
        "id": "oT9P6FC15Bat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Initialize inputs (4 neurons in input layer)\n",
        "inputs = np.array([1, 2, 3, 2.5])\n",
        "\n",
        "# Random weights for the connections between input and hidden layer (4 input neurons, 3 hidden neurons)\n",
        "weights_input_hidden = np.random.randn(4, 3)\n",
        "\n",
        "# Random biases for hidden layer (3 neurons in the hidden layer)\n",
        "bias_hidden = np.random.randn(3)\n",
        "\n",
        "# Random weights for the connections between hidden layer and output layer (3 hidden neurons, 1 output neuron)\n",
        "weights_hidden_output = np.random.randn(3, 1)\n",
        "\n",
        "# Random bias for the output layer\n",
        "bias_output = np.random.randn(1)\n",
        "\n",
        "# Activation function (ReLU)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "# Forward pass for hidden layer\n",
        "hidden_layer_input = np.dot(inputs, weights_input_hidden) + bias_hidden\n",
        "hidden_layer_output = relu(hidden_layer_input)\n",
        "\n",
        "# Forward pass for output layer\n",
        "output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
        "output_layer_output = relu(output_layer_input)\n",
        "\n",
        "print(f\"Input: {inputs}\")\n",
        "print(f\"Hidden layer output: {hidden_layer_output}\")\n",
        "print(f\"Output layer output: {output_layer_output}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jo6gxMzF5Clm",
        "outputId": "5dbe37aa-b8d1-4e06-be3a-c543cc2fe413"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: [1.  2.  3.  2.5]\n",
            "Hidden layer output: [0.         0.         3.79416654]\n",
            "Output layer output: [6.00227052]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Initialization of Inputs and Random Parameters\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import numpy as np: This imports the NumPy library, which is essential for numerical operations in Python, especially for matrix manipulations. We’ll use it for matrix multiplication (np.dot()), random number generation, and other operations.\n",
        "\n",
        "# Initialize inputs (4 neurons in input layer)\n",
        "inputs = np.array([1, 2, 3, 2.5])\n",
        "\n",
        "\n",
        "inputs = np.array([1, 2, 3, 2.5]):\n",
        "\n",
        "This represents the input vector with 4 values. These are the values fed into the neural network. For simplicity, the values are set to [1, 2, 3, 2.5], but in real-world scenarios, these could be feature values of a dataset.\n",
        "\n",
        "# Random weights for the connections between input and hidden layer (4 input neurons, 3 hidden neurons)\n",
        "weights_input_hidden = np.random.randn(4, 3)\n",
        "\n",
        "\n",
        "weights_input_hidden = np.random.randn(4, 3):\n",
        "\n",
        "This initializes the weights between the input layer and the hidden layer.\n",
        "\n",
        "There are 4 input neurons and 3 hidden neurons. Hence, the weight matrix is of shape (4, 3), meaning it has 4 rows (one for each input neuron) and 3 columns (one for each hidden neuron).\n",
        "\n",
        "np.random.randn(4, 3) generates random numbers from a standard normal distribution (mean = 0, standard deviation = 1) for each weight.\n",
        "\n",
        "# Random biases for hidden layer (3 neurons in the hidden layer)\n",
        "bias_hidden = np.random.randn(3)\n",
        "\n",
        "\n",
        "bias_hidden = np.random.randn(3):\n",
        "\n",
        "This initializes the bias values for the hidden layer. There are 3 hidden neurons, so the bias vector has 3 values.\n",
        "\n",
        "Each bias is initialized randomly, again using the standard normal distribution.\n",
        "\n",
        "# Random weights for the connections between hidden layer and output layer (3 hidden neurons, 1 output neuron)\n",
        "weights_hidden_output = np.random.randn(3, 1)\n",
        "\n",
        "\n",
        "weights_hidden_output = np.random.randn(3, 1):\n",
        "\n",
        "This initializes the weights between the hidden layer and the output layer.\n",
        "\n",
        "There are 3 hidden neurons and 1 output neuron, so the weight matrix has a shape of (3, 1).\n",
        "\n",
        "# Random bias for the output layer\n",
        "bias_output = np.random.randn(1)\n",
        "\n",
        "\n",
        "bias_output = np.random.randn(1):\n",
        "\n",
        "This initializes the bias for the output layer. Since there is only 1 output neuron, we have a single random bias value.\n",
        "\n",
        "Step 2: Define the ReLU Activation Function\n",
        "# Activation function (ReLU)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def relu(x): defines the ReLU (Rectified Linear Unit) activation function.\n",
        "\n",
        "np.maximum(0, x): The ReLU function outputs the maximum of 0 or the input x. It simply replaces any negative values with 0, while leaving positive values unchanged.\n",
        "\n",
        "This function is commonly used in neural networks because of its simplicity and ability to help mitigate the vanishing gradient problem during backpropagation.\n",
        "\n",
        "Step 3: Forward Pass Through the Network\n",
        "\n",
        "Now, we compute the outputs by performing a forward pass through the network. This involves calculating the activations of the hidden layer neurons and the output layer neuron.\n",
        "\n",
        "Hidden Layer\n",
        "# Forward pass for hidden layer\n",
        "hidden_layer_input = np.dot(inputs, weights_input_hidden) + bias_hidden\n",
        "\n",
        "\n",
        "hidden_layer_input = np.dot(inputs, weights_input_hidden) + bias_hidden:\n",
        "\n",
        "np.dot(inputs, weights_input_hidden): This performs a dot product between the input vector (inputs) and the weight matrix (weights_input_hidden), which calculates the weighted sum for each hidden neuron.\n",
        "\n",
        "After the dot product, we add the bias for each hidden neuron (i.e., bias_hidden).\n",
        "\n",
        "The result is the input to the hidden layer neurons.\n",
        "\n",
        "hidden_layer_output = relu(hidden_layer_input)\n",
        "\n",
        "\n",
        "hidden_layer_output = relu(hidden_layer_input): This applies the ReLU activation function to the input of the hidden layer, transforming the input into the output of the hidden layer neurons.\n",
        "\n",
        "Output Layer\n",
        "# Forward pass for output layer\n",
        "output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output\n",
        "\n",
        "\n",
        "output_layer_input = np.dot(hidden_layer_output, weights_hidden_output) + bias_output:\n",
        "\n",
        "Here, we calculate the weighted sum for the output neuron by performing a dot product between the output of the hidden layer (hidden_layer_output) and the weight matrix (weights_hidden_output).\n",
        "\n",
        "We add the bias for the output layer (bias_output) to this sum.\n",
        "\n",
        "output_layer_output = relu(output_layer_input)\n",
        "\n",
        "\n",
        "output_layer_output = relu(output_layer_input): Finally, we apply the ReLU activation to the output layer's input, resulting in the final output of the neural network.\n",
        "\n",
        "Step 4: Print the Results\n",
        "print(f\"Input: {inputs}\")\n",
        "print(f\"Hidden layer output: {hidden_layer_output}\")\n",
        "print(f\"Output layer output: {output_layer_output}\")\n",
        "\n",
        "\n",
        "print(f\"Input: {inputs}\"): Prints the input values.\n",
        "\n",
        "print(f\"Hidden layer output: {hidden_layer_output}\"): Prints the output from the hidden layer neurons.\n",
        "\n",
        "print(f\"Output layer output: {output_layer_output}\"): Prints the final output from the output layer."
      ],
      "metadata": {
        "id": "yd4HdlLO-ybt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example-3: Using loop option to get the weights, inputs and biases then feed into the neurons."
      ],
      "metadata": {
        "id": "i5efUhZW-5W1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is more complex then previous one. It is variation of the previous code.\n",
        "# I use a loop to get the weights, inputs and biases then feed into the neurons.\n",
        "inputs = [1, 2, 3, 2.5]\n",
        "weights = [\n",
        "    [0.2, 0.8, -0.5, 1],\n",
        "    [0.5, -0.91, 0.26, -0.5],\n",
        "    [-0.26, -0.27, 0.17, 0.87]\n",
        "]\n",
        "biases = [2, 3, 0.5]\n",
        "\n",
        "# Output of the current layer\n",
        "layer_outputs = []\n",
        "\n",
        "\n",
        "# For each neuron\n",
        "for neuron_weights, neuron_bias in zip(weights, biases):\n",
        "    # Zeroed output of the given neuron\n",
        "    neuron_output = 0\n",
        "\n",
        "    # For each input and weight to the neuron\n",
        "    for n_input, weight in zip(inputs, neuron_weights):\n",
        "        # Multiply this input by the associated weight\n",
        "        # and add to the neuron's output variable\n",
        "\n",
        "        neuron_output += n_input * weight\n",
        "\n",
        "    # Add bias\n",
        "    neuron_output += neuron_bias\n",
        "\n",
        "    # Put the neuron's result into the layer's output list\n",
        "    layer_outputs.append(neuron_output)\n",
        "\n",
        "#print(layer_outputs)\n",
        "w_out=[0.2, 0.8, -0.5]\n",
        "biase_out=0.5\n",
        "final_output=(layer_outputs[0]*w_out[0]+layer_outputs[1]*w_out[1]+layer_outputs[2]*w_out[2])+biase_out\n",
        "print(final_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pva21nDj446m",
        "outputId": "bf0e1d25-d290-4ae2-a61e-651f79d682b3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example-4: Using numpy library to calculate dot product in the neural network."
      ],
      "metadata": {
        "id": "QyFTMzz1AFfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# previous probrams with numpy library, to make it faster and easily programmable.\n",
        "import numpy as np\n",
        "\n",
        "inputs = [1.0, 2.0, 3.0, 2.5]\n",
        "weights = [0.2, 0.8, -0.5, 1.0]\n",
        "bias = 2.0\n",
        "\n",
        "outputs = np.dot(weights, inputs) + bias\n",
        "print(outputs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9AevEu7-9IV",
        "outputId": "f1f92f72-3d04-4502-9470-0724213468ed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy library to make the complex code into simple one.\n",
        "inputs = [1, 2, 3, 2.5]\n",
        "weights = [\n",
        "    [0.2, 0.8, -0.5, 1],\n",
        "    [0.5, -0.91, 0.26, -0.5],\n",
        "    [-0.26, -0.27, 0.17, 0.87]\n",
        "]\n",
        "biases = [2, 3, 0.5]\n",
        "\n",
        "outputs = np.dot(weights, inputs) + biases\n",
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZYynCEC5A1d",
        "outputId": "b72889ba-129b-48e5-acee-8c608c90b3b6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.8   1.21  2.385]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-XpZM_EJAWqv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}